{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnM5tTHxv+6bMyEfZQVx+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAlkam/Mortality-Risk-Prediction-in-Alzheimer-s-Disease-ED-Encounters/blob/main/GitHub_version_of_Final_version_of_Red_flagging_risky_diseases_for_mortality_NEDS_2012_2014_balanced_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Top 30 ICD-9 Diagnosis Codes Among Alzheimerâ€™s Patients: Frequency Analysis with Code Descriptions**"
      ],
      "metadata": {
        "id": "qT7dq4pTuqkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Colab cell â€“ Top-30 ICD-9 codes for AD, with descriptions\n",
        "# ============================================================\n",
        "# 1) Install Stata reader\n",
        "!pip install -q pyreadstat\n",
        "\n",
        "# 2) Imports\n",
        "import pandas as pd, re\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Upload files\n",
        "#    Pick your .dta file (required) and the ICD-9 Excel workbook (optional)\n",
        "print(\"â¬†ï¸  Choose your Stata dataset (.dta) *and* the ICD-9 description \")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Identify paths\n",
        "try:\n",
        "    stata_path = next(fp for fp in uploaded if fp.lower().endswith('.dta'))\n",
        "except StopIteration:\n",
        "    raise RuntimeError(\"âŒ No .dta file uploaded â€“ script cannot continue.\")\n",
        "\n",
        "desc_paths = [fp for fp in uploaded if fp.lower().endswith(('.xls', '.xlsx'))]\n",
        "has_desc  = bool(desc_paths)\n",
        "if has_desc:\n",
        "    desc_path = desc_paths[0]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Read Stata dataset\n",
        "df_raw = pd.read_stata(stata_path)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Read ICD-9 description workbook (if provided) and build lookup\n",
        "icd9_lookup = {}\n",
        "if has_desc:\n",
        "    df_desc = pd.read_excel(desc_path, sheet_name=0)\n",
        "\n",
        "    # Helper: find a column matching any regex in `patterns`\n",
        "    def find_col(patterns, cols):\n",
        "        for pat in patterns:\n",
        "            for c in cols:\n",
        "                if re.fullmatch(pat, c, flags=re.I):\n",
        "                    return c\n",
        "        return None\n",
        "\n",
        "    # Patterns for common header names\n",
        "    patterns_code  = [r'diag.*code', r'dx.*code', r'.*code', r'icd.*']\n",
        "    patterns_long  = [r'long.*(desc|description)', r'(desc|description).*long']\n",
        "    patterns_short = [r'short.*(desc|description)', r'(desc|description).*short']\n",
        "\n",
        "    code_col  = find_col(patterns_code,  df_desc.columns)\n",
        "    long_col  = find_col(patterns_long,  df_desc.columns)\n",
        "    short_col = find_col(patterns_short, df_desc.columns)\n",
        "\n",
        "    if not code_col:\n",
        "        raise ValueError(\"Couldn't identify a column containing the ICD-9 code \"\n",
        "                         \"in the workbook.\")\n",
        "\n",
        "    if not long_col and short_col:      # fall back to short description only\n",
        "        long_col = short_col\n",
        "\n",
        "    if not long_col:\n",
        "        print(\"âš ï¸  No description columns found â€“ tables will list codes only.\")\n",
        "    else:\n",
        "        # Normalise: lower-case, strip spaces/dots\n",
        "        df_desc['key'] = df_desc[code_col].astype(str)\\\n",
        "                             .str.lower().str.replace(r'\\.', '', regex=True).str.strip()\n",
        "        df_desc['description'] = df_desc[long_col]\n",
        "        icd9_lookup = dict(zip(df_desc['key'], df_desc['description']))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Settings\n",
        "ALZ_FLAG, DEATH_FLAG = 'alz', 'died_binary'      # change if your column names differ\n",
        "dx_cols = [c for c in df_raw.columns             # assume dx1-dx30, case-insensitive\n",
        "           if re.fullmatch(r'dx\\d+', c, flags=re.I)]\n",
        "\n",
        "if not dx_cols:\n",
        "    raise RuntimeError(\"No diagnosis columns (dx1, dx2, â€¦) found in the dataset.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8) Helper to build top-N table\n",
        "def top_codes(sub_df, n=30):\n",
        "    # Stack dx1-dx30 into one long Series of codes\n",
        "    codes = (\n",
        "        sub_df[dx_cols]\n",
        "        .astype(str)\n",
        "        .stack()                       # turn wide â†’ long\n",
        "        .str.strip()\n",
        "        .str.lower()\n",
        "        .replace({'nan': ''})\n",
        "    )\n",
        "    # Normalise format: remove periods\n",
        "    codes = codes.str.replace(r'\\.', '', regex=True)\n",
        "    counts = Counter([c for c in codes if c])     # exclude blanks\n",
        "    tbl = pd.DataFrame(counts.most_common(n), columns=['ICD9_Code', 'Frequency'])\n",
        "    if icd9_lookup:\n",
        "        tbl['Description'] = tbl['ICD9_Code'].map(icd9_lookup)\\\n",
        "                                             .fillna('[description missing]')\n",
        "    return tbl\n",
        "\n",
        "# Only Alzheimerâ€™s visits\n",
        "df_ad = df_raw[df_raw[ALZ_FLAG] == 1].copy()\n",
        "\n",
        "tables = {\n",
        "    'ALL_AD_VISITS':       top_codes(df_ad),\n",
        "    'DECEASED_AD_VISITS':  top_codes(df_ad[df_ad[DEATH_FLAG] == 1]),\n",
        "    'SURVIVING_AD_VISITS': top_codes(df_ad[df_ad[DEATH_FLAG] == 0]),\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 9) Print tables and save CSVs\n",
        "for title, tbl in tables.items():\n",
        "    print(f\"\\n=== {title.replace('_', ' ')} ===\")\n",
        "    print(tbl.to_string(index=False))\n",
        "\n",
        "    fname = f\"{title.lower()}.csv\"\n",
        "    tbl.to_csv(fname, index=False)\n",
        "\n",
        "print(\"\\nâœ“ Finished. The three CSV files are saved in the Colab file browser \"\n",
        "      \"on the left (ALL_AD_VISITS.csv, DECEASED_AD_VISITS.csv, SURVIVING_AD_VISITS.csv).\")\n"
      ],
      "metadata": {
        "id": "togttH-BzXbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Risk Analysis of ICD-9 Codes in Alzheimerâ€™s Patients: Prevalence, Risk Ratios, Confidence Intervals, and p-Values**"
      ],
      "metadata": {
        "id": "E71mmJrIvNo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Cell 2 â€“ prevalence, risk ratios & p-values\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "\n",
        "deaths_total = (df_ad[DEATH_FLAG] == 1).sum()\n",
        "surv_total   = (df_ad[DEATH_FLAG] == 0).sum()\n",
        "\n",
        "def code_stats(code_raw):\n",
        "    \"\"\"Return stats tuple for a single ICD-9 code string.\"\"\"\n",
        "    code = str(code_raw).strip().lower().replace('.', '')\n",
        "    if code in ('', 'nan'):\n",
        "        return None            # skip empty cells\n",
        "\n",
        "    deaths_with = ((df_ad[DEATH_FLAG] == 1) &\n",
        "                   (df_ad[dx_cols].eq(code).any(axis=1))).sum()\n",
        "    surv_with   = ((df_ad[DEATH_FLAG] == 0) &\n",
        "                   (df_ad[dx_cols].eq(code).any(axis=1))).sum()\n",
        "\n",
        "    # Skip codes that never occur at all\n",
        "    if (deaths_with + surv_with) == 0:\n",
        "        return None\n",
        "\n",
        "    deaths_without = deaths_total - deaths_with\n",
        "    surv_without   = surv_total   - surv_with\n",
        "    table = np.array([[deaths_with, deaths_without],\n",
        "                      [surv_with,   surv_without]])\n",
        "\n",
        "    # ---------- prevalence ----------\n",
        "    prev_death = deaths_with / deaths_total\n",
        "    prev_surv  = surv_with   / surv_total\n",
        "\n",
        "    # ---------- p-value ----------\n",
        "    try:\n",
        "        chi2, p_val, _, _ = chi2_contingency(table, correction=True)\n",
        "    except ValueError:\n",
        "        # fall back to Fisherâ€™s exact\n",
        "        _, p_val = fisher_exact(table, alternative='two-sided')\n",
        "\n",
        "    # ---------- RR + 95 % CI (Haldaneâ€“Anscombe) ----------\n",
        "    # add 0.5 to each cell only if a zero is present\n",
        "    if (table == 0).any():\n",
        "        table_adj = table + 0.5\n",
        "        rr = (table_adj[0,0] / table_adj[0].sum()) / (table_adj[1,0] / table_adj[1].sum())\n",
        "        se = np.sqrt(1/table_adj[0,0] + 1/table_adj[1,0]\n",
        "                     - 1/table_adj[0].sum() - 1/table_adj[1].sum())\n",
        "    else:\n",
        "        rr = prev_death / prev_surv if prev_surv else np.inf\n",
        "        se = np.sqrt((1/deaths_with) - (1/deaths_total)\n",
        "                     + (1/surv_with) - (1/surv_total))\n",
        "\n",
        "    ci_low, ci_high = np.exp(np.log(rr) - 1.96*se), np.exp(np.log(rr) + 1.96*se)\n",
        "\n",
        "    return (code, deaths_with, surv_with,\n",
        "            prev_death*100, prev_surv*100,\n",
        "            rr, ci_low, ci_high, p_val)\n",
        "\n",
        "# ---------- build the table ----------\n",
        "records = filter(None, (code_stats(code)\n",
        "                        for code in pd.unique(df_ad[dx_cols].values.ravel('K'))))\n",
        "\n",
        "cols = ['ICD9_Code', 'Deaths_with', 'Surv_with',\n",
        "        'Prev_death_%', 'Prev_surv_%',\n",
        "        'RiskRatio', 'CI_low', 'CI_high', 'p_value']\n",
        "rr_df = pd.DataFrame.from_records(records, columns=cols)\n",
        "\n",
        "# attach description if available\n",
        "if icd9_lookup:\n",
        "    rr_df['Description'] = rr_df['ICD9_Code'].map(icd9_lookup).fillna('')\n",
        "\n",
        "# sort & display\n",
        "rr_df = rr_df.sort_values('RiskRatio', ascending=False)\n",
        "print(\"\\n=== Risk ratio table (top 50) ===\")\n",
        "print(rr_df.head(50).to_string(index=False, formatters={\n",
        "    'Prev_death_%': '{:.1f}'.format,\n",
        "    'Prev_surv_%':  '{:.1f}'.format,\n",
        "    'RiskRatio':    '{:.2f}'.format,\n",
        "    'CI_low':       '{:.2f}'.format,\n",
        "    'CI_high':      '{:.2f}'.format,\n",
        "    'p_value':      '{:.2e}'.format}))\n",
        "\n",
        "# save full output\n",
        "rr_df.to_csv('icd9_risk_ratios_full.csv', index=False)\n",
        "print(\"\\nâœ“ Saved icd9_risk_ratios_full.csv for detailed review.\")\n"
      ],
      "metadata": {
        "id": "iY5_gs4i0jtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating and Saving Top 25 High-Risk ICD-9 Codes Table for Alzheimerâ€™s ED Mortality Analysis**"
      ],
      "metadata": {
        "id": "VYBOFLQSvqDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieves or loads the full risk ratio dataset,\n",
        "\n",
        "Filters the top 25 ICD-9 codes based on mortality risk,\n",
        "\n",
        "Formats them for clarity,\n",
        "\n",
        "Displays and exports the result for manuscript use."
      ],
      "metadata": {
        "id": "vvt57am53ZTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "#  Colab cell â€” Top-25 red-flag ICD-9 codes table (death vs. survival)\n",
        "# =============================================================\n",
        "import pandas as pd, os\n",
        "from IPython.display import display\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1.  Get the full risk-ratio dataframe\n",
        "#     â€¢ If rr_df is still in memory, use it.\n",
        "#     â€¢ Otherwise read the CSV we saved earlier.\n",
        "# -------------------------------------------------------------\n",
        "if 'rr_df' in globals():\n",
        "    df = rr_df.copy()\n",
        "elif os.path.exists('icd9_risk_ratios_full.csv'):\n",
        "    df = pd.read_csv('icd9_risk_ratios_full.csv')\n",
        "else:\n",
        "    raise RuntimeError(\"âŒ  No rr_df object or 'icd9_risk_ratios_full.csv' file found.\\n\"\n",
        "                       \"    Run the risk-ratio cell first, then rerun this one.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2.  Select the Top 25 rows by descending risk ratio\n",
        "# -------------------------------------------------------------\n",
        "top_n  = 25\n",
        "top25  = df.sort_values('RiskRatio', ascending=False).head(top_n).copy()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3.  Nicely format numeric columns for readability\n",
        "# -------------------------------------------------------------\n",
        "fmt_map = {\n",
        "    'Prev_death_%': '{:.1f}',\n",
        "    'Prev_surv_%':  '{:.1f}',\n",
        "    'RiskRatio':    '{:.2f}',\n",
        "    'CI_low':       '{:.2f}',\n",
        "    'CI_high':      '{:.2f}',\n",
        "    'p_value':      '{:.2e}',\n",
        "}\n",
        "for col, fmt in fmt_map.items():\n",
        "    if col in top25.columns:\n",
        "        top25[col] = top25[col].map(lambda x: fmt.format(x))\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4.  Re-order columns in a logical order (skip if missing)\n",
        "# -------------------------------------------------------------\n",
        "order = ['ICD9_Code', 'Description', 'Deaths_with', 'Surv_with',\n",
        "         'Prev_death_%', 'Prev_surv_%',\n",
        "         'RiskRatio', 'CI_low', 'CI_high', 'p_value']\n",
        "top25 = top25[[c for c in order if c in top25.columns]]\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5.  Display the table\n",
        "#     (plain display avoids hide_index issues on older pandas)\n",
        "# -------------------------------------------------------------\n",
        "display(top25)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6.  Save a CSV you can download for the manuscript\n",
        "# -------------------------------------------------------------\n",
        "fname = 'Top25_redflag_ICD9_table.csv'\n",
        "top25.to_csv(fname, index=False)\n",
        "print(f\"\\nâœ“ Saved '{fname}'.  Download it from the file pane on the left.\")\n"
      ],
      "metadata": {
        "id": "P0g9M3dD7Qq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculating and Exporting Risk Ratios (RR) and Odds Ratios (OR) for All (25) ICD-9 Codes in Alzheimerâ€™s ED Data**"
      ],
      "metadata": {
        "id": "OY3PPZ7FK_T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reconstructs 2Ã—2 tables for each ICD-9 code,\n",
        "\n",
        "Calculates both RR and OR (with 95% confidence intervals),\n",
        "\n",
        "Displays the top 25 by RR,\n",
        "\n",
        "Saves a complete table for publication or further analysis."
      ],
      "metadata": {
        "id": "L_vi_w5l31ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "#  Colab cell â€“ RR *and* OR for every ICD-9 code\n",
        "# =============================================================\n",
        "import numpy as np, pandas as pd, os\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1.  Load the rr_df (risk-ratio) DataFrame if it exists;\n",
        "#     otherwise read the CSV produced earlier.\n",
        "# -------------------------------------------------------------\n",
        "if 'rr_df' in globals():\n",
        "    df_all = rr_df.copy()\n",
        "elif os.path.exists('icd9_risk_ratios_full.csv'):\n",
        "    df_all = pd.read_csv('icd9_risk_ratios_full.csv')\n",
        "else:\n",
        "    raise RuntimeError(\"Run the risk-ratio cell first to create icd9_risk_ratios_full.csv.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2.  Reconstruct counts from the stored columns\n",
        "#     (deaths_with, surv_with, total deaths & survivors)\n",
        "# -------------------------------------------------------------\n",
        "deaths_total = (df_ad[DEATH_FLAG] == 1).sum()    # if df_ad still in RAM\n",
        "surv_total   = (df_ad[DEATH_FLAG] == 0).sum()\n",
        "\n",
        "if pd.isna(deaths_total) or pd.isna(surv_total):\n",
        "    raise RuntimeError(\"Need df_ad in memory; if kernel was reset, reload your dataset.\")\n",
        "\n",
        "records = []\n",
        "for _, row in df_all.iterrows():\n",
        "    code         = row['ICD9_Code']\n",
        "    deaths_with  = int(row['Deaths_with'])\n",
        "    surv_with    = int(row['Surv_with'])\n",
        "    deaths_without = deaths_total - deaths_with\n",
        "    surv_without   = surv_total  - surv_with\n",
        "\n",
        "    # --- add 0.5 to every cell if any zero -> Haldane-Anscombe correction\n",
        "    table = np.array([[deaths_with,  deaths_without],\n",
        "                      [surv_with,    surv_without]], dtype=float)\n",
        "    if (table == 0).any():\n",
        "        table += 0.5\n",
        "\n",
        "    # --- RR (already computed, but recalc for completeness)\n",
        "    prev_death = table[0,0] / table[0].sum()\n",
        "    prev_surv  = table[1,0] / table[1].sum()\n",
        "    rr = prev_death / prev_surv\n",
        "\n",
        "    # --- OR\n",
        "    or_val = (table[0,0] * table[1,1]) / (table[0,1] * table[1,0])\n",
        "\n",
        "    # 95 % CI for OR (Woolf log method)\n",
        "    se_log_or = np.sqrt(1/table[0,0] + 1/table[0,1] + 1/table[1,0] + 1/table[1,1])\n",
        "    ci_low_or, ci_high_or = np.exp(np.log(or_val) - 1.96*se_log_or), \\\n",
        "                            np.exp(np.log(or_val) + 1.96*se_log_or)\n",
        "\n",
        "    # Fisher exact p for reference\n",
        "    _, p_two = fisher_exact(table, alternative='two-sided')\n",
        "\n",
        "    records.append({\n",
        "        'ICD9_Code': code,\n",
        "        'Description': row.get('Description', ''),\n",
        "        'Deaths_with': int(row['Deaths_with']),\n",
        "        'Surv_with':   int(row['Surv_with']),\n",
        "        'RR':  rr,\n",
        "        'OR':  or_val,\n",
        "        'OR_CI_low':  ci_low_or,\n",
        "        'OR_CI_high': ci_high_or,\n",
        "        'p_value': p_two\n",
        "    })\n",
        "\n",
        "rr_or_df = pd.DataFrame(records)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3.  Nice formatting & display\n",
        "# -------------------------------------------------------------\n",
        "fmt = {'RR': '{:.2f}', 'OR': '{:.2f}', 'OR_CI_low': '{:.2f}', 'OR_CI_high': '{:.2f}', 'p_value': '{:.2e}'}\n",
        "for c, f in fmt.items(): rr_or_df[c] = rr_or_df[c].map(lambda x: f.format(x))\n",
        "\n",
        "# Show top 25 by RR (or change to 'OR' to sort by odds ratio)\n",
        "top25_rr = rr_or_df.sort_values('RR', ascending=False).head(25).reset_index(drop=True)\n",
        "from IPython.display import display\n",
        "display(top25_rr)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4.  Save full table to disk\n",
        "# -------------------------------------------------------------\n",
        "rr_or_df.to_csv('ICD9_RR_OR_full.csv', index=False)\n",
        "print(\"âœ“ Saved 'ICD9_RR_OR_full.csv'.  You can download it from the file pane.\")\n"
      ],
      "metadata": {
        "id": "Dp0Y2u8bK97l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full ML Pipeline for Mortality Prediction in Alzheimerâ€™s ED Visits: Model Training, Evaluation, and SHAP Interpretation with ICD-9 Codes**"
      ],
      "metadata": {
        "id": "AFqbQ7iLxIh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading and preprocessing a diagnosis-based dataset\n",
        "\n",
        "Training four predictive models (Logistic Regression, Random Forest, Gradient Boosting, XGBoost)\n",
        "\n",
        "Evaluating model performance with accuracy, precision, recall, F1, and AUROC\n",
        "\n",
        "Interpreting the best model (XGBoost) using SHAP\n",
        "\n",
        "Visualizing top ICD-9 features linked to mortality in Alzheimerâ€™s patients\n",
        "\n",
        "Saving publication-quality SHAP plots"
      ],
      "metadata": {
        "id": "4ip8vrQU35X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "#  Mortality Prediction in AD-Related ED Visits (NEDS 2012-2014)\n",
        "#  â€¢ Train / Validation / Test (60 / 20 / 20)\n",
        "#  â€¢ Four candidate models â†’ choose best (AUROC on validation)\n",
        "#  â€¢ Final evaluation on untouched test set\n",
        "#  â€¢ SHAP feature interpretation with pretty labels\n",
        "#  â€¢ Saves model + label map       â€” ready for reproducible research\n",
        "# ===============================================================\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 0 â–¸ Install required packages (comment out if already installed)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "!pip install -q pyreadstat scikit-learn==1.5.0 xgboost==2.0.0 shap matplotlib pandas numpy joblib\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1 â–¸ Imports\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os, re, copy, joblib\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import pyreadstat, shap\n",
        "from google.colab import files                          # remove if not in Colab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, roc_curve)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2 â–¸ Data upload & preprocessing\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"â¬†ï¸  Upload balanced NEDS .dta file (must include *_bin flags and died_binary)\")\n",
        "uploaded  = files.upload()                               # switch to input() if not using Colab\n",
        "dta_file  = next(iter(uploaded))\n",
        "with open(dta_file, \"wb\") as f: f.write(uploaded[dta_file])\n",
        "\n",
        "df, _ = pyreadstat.read_dta(dta_file)\n",
        "print(f\"âœ… Loaded {df.shape[0]:,} encounters\")\n",
        "\n",
        "# Select binary comorbidity flags\n",
        "bin_vars = [c for c in df.columns\n",
        "            if c.endswith('_bin') and df[c].dropna().nunique() <= 2]\n",
        "dfm = df[bin_vars + ['died_binary']].dropna()\n",
        "X, y = dfm[bin_vars], dfm['died_binary']\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3 â–¸ Stratified Train / Val / Test split (60 / 20 / 20)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.25, stratify=y_tmp, random_state=42)\n",
        "\n",
        "print(f\"Split sizes  âœ  Train: {len(X_train)}  Val: {len(X_val)}  Test: {len(X_test)}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 4 â–¸ Define candidate models\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
        "    \"Random Forest\":       RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
        "    \"Gradient Boosting\":   GradientBoostingClassifier(random_state=42),\n",
        "    \"XGBoost\":             XGBClassifier(\n",
        "                              use_label_encoder=False, eval_metric='logloss',\n",
        "                              n_estimators=400, max_depth=5, learning_rate=0.05,\n",
        "                              subsample=0.8, colsample_bytree=0.8,\n",
        "                              random_state=42)\n",
        "}\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 5 â–¸ Fit on Train, evaluate on Validation\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "perf_records = []\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    val_pred = clf.predict(X_val)\n",
        "    val_prob = clf.predict_proba(X_val)[:, 1]\n",
        "    perf_records.append({\n",
        "        \"Model\":       name,\n",
        "        \"Val_AUROC\":   roc_auc_score(y_val, val_prob),\n",
        "        \"Val_Prec\":    precision_score(y_val, val_pred),\n",
        "        \"Val_Rec\":     recall_score(y_val, val_pred),\n",
        "        \"Val_F1\":      f1_score(y_val, val_pred)\n",
        "    })\n",
        "\n",
        "perf_df = pd.DataFrame(perf_records).sort_values(\"Val_AUROC\", ascending=False)\n",
        "print(\"\\nValidation metrics\")\n",
        "print(perf_df.to_string(index=False))\n",
        "\n",
        "best_name = perf_df.iloc[0][\"Model\"]\n",
        "best_clf_val = models[best_name]            # already fitted on Train\n",
        "print(f\"\\nğŸ† Selected model: {best_name}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 6 â–¸ Re-train winner on Train+Val, evaluate on Test\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "X_tv, y_tv = pd.concat([X_train, X_val]), pd.concat([y_train, y_val])\n",
        "best_clf_tv = copy.deepcopy(best_clf_val).fit(X_tv, y_tv)\n",
        "\n",
        "test_pred = best_clf_tv.predict(X_test)\n",
        "test_prob = best_clf_tv.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nHeld-out TEST metrics\")\n",
        "print(f\"  AUROC    : {roc_auc_score(y_test, test_prob):.3f}\")\n",
        "print(f\"  Accuracy : {accuracy_score(y_test, test_pred):.3f}\")\n",
        "print(f\"  Precision: {precision_score(y_test, test_pred):.3f}\")\n",
        "print(f\"  Recall   : {recall_score(y_test, test_pred):.3f}\")\n",
        "print(f\"  F1       : {f1_score(y_test, test_pred):.3f}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 7 â–¸ ROC curves on Test (all models)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "plt.figure(figsize=(7,6))\n",
        "for name, clf in models.items():\n",
        "    # fit non-winners on Train+Val for fair test comparison\n",
        "    if name != best_name:\n",
        "        clf = copy.deepcopy(clf).fit(X_tv, y_tv)\n",
        "    prob = clf.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC {roc_auc_score(y_test, prob):.3f})\")\n",
        "plt.plot([0,1],[0,1],'k--', lw=0.8)\n",
        "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC curves â€“ Test set\"); plt.legend(); plt.grid(True); plt.show()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 8 â–¸ SHAP interpretability with pretty labels\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def prettify(col):\n",
        "    return re.sub(r'_bin$', '', col).replace('_', ' ').title()\n",
        "\n",
        "nice = {c: prettify(c) for c in X.columns}\n",
        "X_tv_nice   = X_tv.rename(columns=nice)\n",
        "X_test_nice = X_test.rename(columns=nice)\n",
        "\n",
        "if best_name in [\"Gradient Boosting\", \"Random Forest\", \"XGBoost\"]:\n",
        "    explainer = shap.TreeExplainer(best_clf_tv)\n",
        "    shap_vals = explainer.shap_values(X_test_nice)\n",
        "else:\n",
        "    explainer = shap.LinearExplainer(best_clf_tv, X_tv_nice)\n",
        "    shap_vals = explainer.shap_values(X_test_nice)\n",
        "\n",
        "shap.summary_plot(shap_vals, X_test_nice, plot_type='dot', max_display=20)\n",
        "shap.summary_plot(shap_vals, X_test_nice, plot_type='bar', max_display=20)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 9 â–¸ Save artefacts for reproducibility\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "model_path = f\"models/{best_name.lower().replace(' ','_')}_alz.pkl\"\n",
        "joblib.dump(best_clf_tv, model_path)                 # final model\n",
        "joblib.dump(nice, \"models/feature_name_map.pkl\")     # for plotting\n",
        "print(f\"\\nğŸ”’ Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "mjSoGRs2I56r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B7xlSg32PICQ"
      }
    }
  ]
}